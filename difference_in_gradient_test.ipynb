{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Численный градиент: \n",
      " [[ 0.01060654]\n",
      " [-0.00932258]\n",
      " [ 0.0044515 ]\n",
      " [-0.02756305]\n",
      " [-0.08094245]]\n",
      "Аналитический градиент: \n",
      " [[ 0.01044397]\n",
      " [-0.013099  ]\n",
      " [ 0.00200913]\n",
      " [-0.02686574]\n",
      " [-0.08020722]]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f88e6c940a484ffe8fb717c46fd55eab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(RadioButtons(description='eps', options=('3', '1', '0.1', '0.001', '0.0001'), value='3')…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6441e321e32e4dd49cb12168128b0e82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(RadioButtons(description='eps', options=('3', '1', '0.1', '0.001', '0.0001'), value='3')…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import mpl_toolkits.mplot3d as p3\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "\n",
    "from functools import partial\n",
    "from ipywidgets import interact, RadioButtons, IntSlider, FloatSlider, Dropdown, BoundedFloatText\n",
    "from numpy.linalg import norm\n",
    "\n",
    "random.seed(42) # начальное состояние генератора случайных чисел, чтобы можно было воспроизводить результаты.\n",
    "\n",
    "#Активационная функция сигмоидального нейрона\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "#Производная активационной функции сигмоидального нейрона\n",
    "def sigmoid_prime(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "#Считает целевую функциюю\n",
    "def J_quadratic(neuron, X, y):\n",
    "    \"\"\"\n",
    "    Оценивает значение квадратичной целевой функции.\n",
    "    Всё как в лекции, никаких хитростей.\n",
    "\n",
    "    neuron - нейрон, у которого есть метод vectorized_forward_pass, предсказывающий значения на выборке X\n",
    "    X - матрица входных активаций (n, m)\n",
    "    y - вектор правильных ответов (n, 1)\n",
    "\n",
    "    Возвращает значение J (число)\n",
    "    \"\"\"\n",
    "\n",
    "    assert y.shape[1] == 1, 'Incorrect y shape'\n",
    "\n",
    "    return 0.5 * np.mean((neuron.vectorized_forward_pass(X) - y) ** 2)\n",
    "\n",
    "#Вычисляет вектор частных производных целевой функции по каждому из предсказаний(используется при расчете градиента)\n",
    "def J_quadratic_derivative(y, y_hat):\n",
    "    \"\"\"\n",
    "    Вычисляет вектор частных производных целевой функции по каждому из предсказаний.\n",
    "    y_hat - вертикальный вектор предсказаний,\n",
    "    y - вертикальный вектор правильных ответов,\n",
    "    \n",
    "    В данном случае функция смехотворно простая, но если мы захотим поэкспериментировать \n",
    "    с целевыми функциями - полезно вынести эти вычисления в отдельный этап.\n",
    "    \n",
    "    Возвращает вектор значений производной целевой функции для каждого примера отдельно.\n",
    "    \"\"\"\n",
    "    \n",
    "    assert y_hat.shape == y.shape and y_hat.shape[1] == 1, 'Incorrect shapes'\n",
    "    \n",
    "    return (y_hat - y) / len(y)\n",
    "    \n",
    "\n",
    "#Рассчитывает градиент(аналитическая производная целевой функции)\n",
    "def compute_grad_analytically(neuron, X, y, J_prime=J_quadratic_derivative):\n",
    "    \"\"\"\n",
    "    Аналитическая производная целевой функции\n",
    "    neuron - объект класса Neuron\n",
    "    X - вертикальная матрица входов формы (n, m), на которой считается сумма квадратов отклонений\n",
    "    y - правильные ответы для примеров из матрицы X\n",
    "    J_prime - функция, считающая производные целевой функции по ответам\n",
    "    \n",
    "    Возвращает вектор размера (m, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Вычисляем активации\n",
    "    # z - вектор результатов сумматорной функции нейрона на разных примерах\n",
    "    \n",
    "    z = neuron.summatory(X)\n",
    "    y_hat = neuron.activation(z)\n",
    "\n",
    "    # Вычисляем нужные нам частные производные\n",
    "    dy_dyhat = J_prime(y, y_hat)\n",
    "    dyhat_dz = neuron.activation_function_derivative(z)\n",
    "    \n",
    "    \n",
    "    # осознайте эту строчку:\n",
    "    dz_dw = X\n",
    "\n",
    "    \n",
    "\n",
    "    # а главное, эту:\n",
    "    grad = ((dy_dyhat * dyhat_dz).T).dot(dz_dw)\n",
    "    \n",
    "    # можно было написать в два этапа. Осознайте, почему получается одно и то же\n",
    "    #2)grad_matrix = dy_dyhat * dyhat_dz * dz_dw\n",
    "    #2)grad = np.sum(grad_matrix, axis=0)\n",
    "    \n",
    "    # Сделаем из горизонтального вектора вертикальный\n",
    "    \n",
    "    \n",
    "    grad = grad.T\n",
    "    #2)grad.shape = (5,1)\n",
    "    \n",
    "    return grad\n",
    "\n",
    "#Провера градиента(численная производная целевой функции)(не самый лучший метод)\n",
    "def compute_grad_numerically(neuron, X, y, J=J_quadratic, eps=10e-2):\n",
    "    initial_cost = J(neuron, X, y)\n",
    "    w_0 = neuron.w\n",
    "    num_grad = np.zeros(w_0.shape)\n",
    "    \n",
    "    for i in range(len(w_0)):\n",
    "        \n",
    "        old_wi = neuron.w[i].copy()\n",
    "        \n",
    "        neuron.w[i] += eps\n",
    "        \n",
    "        num_grad[i] = (J(neuron, X, y) - initial_cost)/eps\n",
    "        \n",
    "        neuron.w[i] = old_wi\n",
    "        \n",
    "    assert np.allclose(neuron.w, w_0), \"МЫ ИСПОРТИЛИ ВЕСА\"\n",
    "    return num_grad\n",
    "\n",
    "\n",
    "'''(Просто другой способ)\n",
    "def compute_grad_numerically_2(neuron, X, y, J=J_quadratic, eps=10e-2):\n",
    "    \n",
    "    w_0 = neuron.w\n",
    "    num_grad = np.zeros(neuron.w.shape)\n",
    "\n",
    "    for i in range(len(neuron.w)):\n",
    "        \n",
    "        oldW = neuron.w[i,0]\n",
    "        neuron.w[i] += eps\n",
    "        result = J(neuron, X, y)\n",
    "        neuron.w[i,0] = oldW\n",
    "        neuron.w[i] -= eps\n",
    "        num_grad[i] = (result - J(neuron, X, y))/(2*eps)\n",
    "        neuron.w[i,0] = oldW\n",
    "        \n",
    "    assert np.allclose(neuron.w, w_0), \"WE SPOILED THE WEIGHT\"\n",
    "    return num_grad'''\n",
    "        \n",
    "#Провера градиента(численная производная целевой функции)(лучше чем первая версия)\n",
    "def compute_grad_numerically_2(neuron, X, y, J=J_quadratic, eps=10e-2):\n",
    "    \n",
    "    w_0 = neuron.w\n",
    "    num_grad = np.zeros(neuron.w.shape)\n",
    "\n",
    "    for i in range(len(neuron.w)):\n",
    "        \n",
    "        for k in range(-1,2,2):\n",
    "            \n",
    "            oldW = neuron.w[i,0]\n",
    "            neuron.w[i] += eps*k\n",
    "            num_grad[i] += J(neuron, X, y)*k\n",
    "            neuron.w[i,0] = oldW\n",
    "        \n",
    "    assert np.allclose(neuron.w, w_0), \"WE SPOILED THE WEIGHT\"\n",
    "    return num_grad/(2*eps)\n",
    "\n",
    "#Функция расчета разницы градиента при 1 методе проверки\n",
    "def print_grad_diff(eps):\n",
    "    num_grad = compute_grad_numerically(N, X, y, J=J_quadratic, eps=float(eps))\n",
    "    an_grad = compute_grad_analytically(N, X, y, J_prime=J_quadratic_derivative)\n",
    "    print(np.linalg.norm(num_grad-an_grad))\n",
    "\n",
    "#Функция расчета разницы градиента при 2 методе проверки\n",
    "def print_grad_diff_2(eps):\n",
    "    num_grad = compute_grad_numerically_2(N, X, y, J=J_quadratic, eps=float(eps))\n",
    "    an_grad = compute_grad_analytically(N, X, y, J_prime=J_quadratic_derivative)\n",
    "    print(np.linalg.norm(num_grad-an_grad))\n",
    "\n",
    "class Neuron:\n",
    "\n",
    "    \n",
    "    def __init__(self, weights, activation_function=sigmoid, activation_function_derivative = sigmoid_prime):\n",
    "        \n",
    "        assert weights.shape[1] == 1, \"Incorrect weight shape\"\n",
    "        \n",
    "        self.w = weights\n",
    "        self.activation_function = activation_function\n",
    "        self.activation_function_derivative = activation_function_derivative\n",
    "    \n",
    "    #Сумматорный метод\n",
    "    def summatory(self, input_matrix):\n",
    "        return input_matrix.dot(self.w)\n",
    "    \n",
    "    #Активационный метод\n",
    "    def activation(self, summatory_activation):\n",
    "        return self.activation_function(summatory_activation)\n",
    "    \n",
    "    #Векторизованный метод рассчета предсказанных значений\n",
    "    def vectorized_forward_pass(self, input_matrix):\n",
    "        return self.activation(self.summatory(input_matrix))\n",
    "    \n",
    "np.random.seed(42)\n",
    "n = 10\n",
    "m = 5\n",
    "\n",
    "X = 20 * np.random.sample((n,m)) - 10\n",
    "y = (np.random.random(n) < 0.5).astype(np.int)[:,np.newaxis]\n",
    "w = 2 * np.random.random((m,1)) - 1\n",
    "\n",
    "\n",
    "N = Neuron(w)\n",
    "\n",
    "num_grad = compute_grad_numerically_2(N, X, y, J=J_quadratic)\n",
    "an_grad = compute_grad_analytically(N, X, y, J_prime = J_quadratic_derivative)\n",
    "\n",
    "print(\"Численный градиент: \\n\", num_grad)\n",
    "print(\"Аналитический градиент: \\n\", an_grad)\n",
    "\n",
    "interact(print_grad_diff,  \n",
    "            eps=RadioButtons(options=[\"3\", \"1\", \"0.1\", \"0.001\", \"0.0001\"]), separator=\" \");\n",
    "\n",
    "interact(print_grad_diff_2, \n",
    "            eps=RadioButtons(options=[\"3\", \"1\", \"0.1\", \"0.001\", \"0.0001\"]), separator=\" \");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
