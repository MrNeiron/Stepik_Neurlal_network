{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "activation L2: \n",
      " [[0.9       0.7109495]] \n",
      "\n",
      "activation L3: \n",
      " [[0.61405267]] \n",
      "\n",
      "error L3: \n",
      " [[-0.09146642]] \n",
      "\n",
      "error L2: \n",
      " [-0.01829328 -0.00751855] \n",
      "\n",
      "dJ_dw1_3:  -0.018293284971249293\n",
      "dJ_dw2_3:  -0.0075185513677826785\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#Активационная функция сигмоидального нейрона\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "    \n",
    "#Производная активационной функции сигмоидального нейрона\n",
    "def sigmoid_prime(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "def _max(x):\n",
    "    return np.max(x,0)\n",
    "\n",
    "def max_prime(x):\n",
    "    return int(x>0)\n",
    "\n",
    "#Вычисляет вектор частных производных целевой функции по каждому из предсказаний(используется при расчете градиента)\n",
    "def J_quadratic_derivative(y, y_hat):\n",
    "\n",
    "    assert y_hat.shape == y.shape and y_hat.shape[1] == 1, 'Incorrect shapes'\n",
    "    \n",
    "    return (y_hat - y) / len(y)\n",
    "    \n",
    "    \n",
    "#Рассчитывает градиент(аналитическая производная целевой функции)\n",
    "def compute_grad_analytically(neuron, X, y, J_prime=J_quadratic_derivative):\n",
    "    \n",
    "    # Вычисляем активации\n",
    "    # z - вектор результатов сумматорной функции нейрона на разных примерах\n",
    "    \n",
    "    z = neuron.summatory(X)\n",
    "    y_hat = neuron.activation(z)\n",
    "\n",
    "    # Вычисляем нужные нам частные производные\n",
    "    dy_dyhat = J_prime(y, y_hat)\n",
    "    dyhat_dz = neuron.activation_function_derivative(z)\n",
    "    \n",
    "    # осознайте эту строчку:\n",
    "    dz_dw = X\n",
    "\n",
    "    \n",
    "\n",
    "    # а главное, эту:\n",
    "    grad = ((dy_dyhat * dyhat_dz).T).dot(dz_dw)\n",
    "    \n",
    "    # можно было написать в два этапа. Осознайте, почему получается одно и то же\n",
    "    #2)grad_matrix = dy_dyhat * dyhat_dz * dz_dw\n",
    "    #2)grad = np.sum(grad_matrix, axis=0)\n",
    "    \n",
    "    # Сделаем из горизонтального вектора вертикальный\n",
    "    \n",
    "    \n",
    "    grad = grad.T\n",
    "    #2)grad.shape = (5,1)\n",
    "    \n",
    "    return grad\n",
    "\n",
    "def compute_error_analytically(neuron, X, y, J_prime=J_quadratic_derivative):\n",
    "     # Вычисляем активации\n",
    "    # z - вектор результатов сумматорной функции нейрона на разных примерах\n",
    "    \n",
    "    z = neuron.summatory(X)\n",
    "    y_hat = neuron.activation(z)\n",
    "\n",
    "    # Вычисляем нужные нам частные производные\n",
    "    dy_dyhat = J_prime(y, y_hat)\n",
    "    dyhat_dz = neuron.activation_function_derivative(z)\n",
    "    \n",
    "\n",
    "    # а главное, эту:\n",
    "    error = (dy_dyhat * dyhat_dz).T\n",
    "\n",
    "    # Сделаем из горизонтального вектора вертикальный\n",
    "    \n",
    "    \n",
    "    error = error.T\n",
    "    \n",
    "    return error\n",
    "\n",
    "class Neuron:\n",
    "\n",
    "    def __init__(self, weights, activation_function = sigmoid, activation_function_derivative = sigmoid_prime):\n",
    "        \n",
    "        #assert weights.shape[1] == 1, \"Incorrect weight shape\"\n",
    "        \n",
    "        self.w = weights\n",
    "        self.activation_function = activation_function\n",
    "        self.activation_function2 = _max\n",
    "        self.activation_function_derivative = activation_function_derivative\n",
    "        self.activation_function_derivative2 = max_prime\n",
    "\n",
    "    #Сумматорный метод\n",
    "    def summatory(self, input_matrix):\n",
    "        return input_matrix.dot(self.w)\n",
    "        \n",
    "    #Активационный метод\n",
    "    def activation(self, summatory_activation):\n",
    "        #print(\"s: \", self.activation_function(summatory_activation))\n",
    "        return self.activation_function(summatory_activation)\n",
    "        \n",
    "    #Векторизованный метод рассчета предсказанных значений(гораздо лучший способ)\n",
    "    def vectorized_forward_pass(self, input_matrix):\n",
    "        return self.activation(self.summatory(input_matrix))\n",
    "    \n",
    "    \n",
    "    #Обучение весов с помощью одного батча(несколько случайных примеров из всех имеющихся примеров)\n",
    "    def update_mini_batch(self, X, y, learning_rate, eps):\n",
    "        \n",
    "        before = J_quadratic(self, X, y)\n",
    "        \n",
    "        #self.w -= learning_rate * compute_grad_analytically(self,X,y)\n",
    "        self.w -= learning_rate * compute_grad_numerically_2(self,X,y)\n",
    "\n",
    "        \n",
    "        return np.abs(before-J_quadratic(self, X, y)) < eps\n",
    "    \n",
    "    \n",
    "    #Обучение весов с помощью батчей\n",
    "    def SGD(self, X, y, batch_size, learning_rate =0.1, eps=1e-6, max_steps = 200):\n",
    "        cur_step = 0\n",
    "        #for _ in range(max_steps):\n",
    "        while cur_step < max_steps:\n",
    "            cur_step += 1\n",
    "            batch = np.random.choice(len(X), batch_size, replace = False)\n",
    "            if self.update_mini_batch(X[batch], y[batch], learning_rate, eps) == 1: \n",
    "                return 1\n",
    "        return 0\n",
    "                \n",
    "\n",
    "def get_error_l3(a, y):\n",
    "    \n",
    "    return (a - y) * a * (1-a)\n",
    "    \n",
    "def get_error_l2(deltas, sums, weights, activation_func = sigmoid_prime):\n",
    "    return (weights.reshape(len(weights),1).dot(deltas.reshape(1,deltas.shape[0])) * activation_func(sums.T)).mean(axis = 1)#более быстрое решение\n",
    "    #return (weights.T.dot(deltas.reshape(1,deltas.shape[0])) * sigmoid_prime(sums.T)).mean(axis = 1)#более быстрое решение\n",
    "    #return (deltas.dot(weights) * sigmoid_prime(sums)).mean(axis = 0).T\n",
    "\n",
    "\n",
    "def FindWeight(l1,l2,l3,y):\n",
    "    \n",
    "    sumsL2 = l2.summatory(l1)\n",
    "    aL2 = np.array((l2.activation_function2(sumsL2[0][0]), l2.activation_function(sumsL2[0][1]))).reshape(1,2)\n",
    "    print(\"activation L2: \\n\", aL2, '\\n')\n",
    "    \n",
    "    sumsL3 = l3.summatory(aL2).T\n",
    "    aL3 = l3.activation_function(sumsL3).T\n",
    "    print(\"activation L3: \\n\", aL3, '\\n')\n",
    "    \n",
    "    errorL3 = get_error_l3(aL3, y)\n",
    "    print(\"error L3: \\n\", errorL3, '\\n')\n",
    "    \n",
    "    errorL2 = np.array((get_error_l2(errorL3, sumsL2[0][0], l3.w[0], activation_func = max_prime),get_error_l2(errorL3, sumsL2[0][1], l3.w[1]))).reshape(2)\n",
    "    print(\"error L2: \\n\", errorL2, '\\n')\n",
    "    \n",
    "    dJ_dw1_3 = l1[0][2]*errorL2[0]\n",
    "    print(\"dJ_dw1_3: \", dJ_dw1_3)\n",
    "    dJ_dw2_3 = l1[0][2]*errorL2[1]\n",
    "    print(\"dJ_dw2_3: \", dJ_dw2_3)\n",
    "    \n",
    "    pass\n",
    "\n",
    "w = np.array([[0.7,0.2,0.7], [0.8 ,0.3 ,0.6]])\n",
    "w = w.T\n",
    "\n",
    "w1 = np.array([[0.2],[0.4]])\n",
    "x = np.array([[0.0,1.0,1.0]])\n",
    "y = np.array([[1]])\n",
    "layer2 = Neuron(w)\n",
    "layer3 = Neuron(w1)\n",
    "\n",
    "FindWeight(x, layer2, layer3, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
