{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    return sigmoid(z)*(1-sigmoid(z))\n",
    "\n",
    "def cost_function(network, test_data, onehot=True):\n",
    "    c = 0\n",
    "    for example, y in test_data:\n",
    "        if not onehot:\n",
    "            y = np.eye(3,1, k=-int(y))\n",
    "        yhat = network.feedforward(example)\n",
    "        c += np.sum((y - yhat)**2)\n",
    "    return c / len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Эпоха 0: 74 / 130\n",
      "Эпоха 1: 83 / 130\n",
      "Эпоха 2: 91 / 130\n",
      "Эпоха 3: 92 / 130\n",
      "Эпоха 4: 95 / 130\n",
      "Эпоха 5: 90 / 130\n",
      "Эпоха 6: 93 / 130\n",
      "Эпоха 7: 95 / 130\n",
      "Эпоха 8: 95 / 130\n",
      "Эпоха 9: 97 / 130\n",
      "Эпоха 10: 98 / 130\n",
      "Эпоха 11: 97 / 130\n",
      "Эпоха 12: 100 / 130\n",
      "Эпоха 13: 100 / 130\n",
      "Эпоха 14: 103 / 130\n",
      "Эпоха 15: 103 / 130\n",
      "Эпоха 16: 103 / 130\n",
      "Эпоха 17: 103 / 130\n",
      "Эпоха 18: 103 / 130\n",
      "Эпоха 19: 103 / 130\n",
      "Эпоха 20: 103 / 130\n",
      "Эпоха 21: 103 / 130\n",
      "Эпоха 22: 103 / 130\n",
      "Эпоха 23: 101 / 130\n",
      "Эпоха 24: 103 / 130\n",
      "Эпоха 25: 103 / 130\n",
      "Эпоха 26: 103 / 130\n",
      "Эпоха 27: 103 / 130\n",
      "Эпоха 28: 104 / 130\n",
      "Эпоха 29: 103 / 130\n",
      "Эпоха 30: 104 / 130\n",
      "Эпоха 31: 104 / 130\n",
      "Эпоха 32: 104 / 130\n",
      "Эпоха 33: 103 / 130\n",
      "Эпоха 34: 103 / 130\n",
      "Эпоха 35: 105 / 130\n",
      "Эпоха 36: 104 / 130\n",
      "Эпоха 37: 103 / 130\n",
      "Эпоха 38: 104 / 130\n",
      "Эпоха 39: 104 / 130\n",
      "Эпоха 40: 103 / 130\n",
      "Эпоха 41: 105 / 130\n",
      "Эпоха 42: 103 / 130\n",
      "Эпоха 43: 104 / 130\n",
      "Эпоха 44: 105 / 130\n",
      "Эпоха 45: 106 / 130\n",
      "Эпоха 46: 103 / 130\n",
      "Эпоха 47: 106 / 130\n",
      "Эпоха 48: 112 / 130\n",
      "Эпоха 49: 103 / 130\n",
      "Эпоха 50: 105 / 130\n",
      "Эпоха 51: 103 / 130\n",
      "Эпоха 52: 105 / 130\n",
      "Эпоха 53: 104 / 130\n",
      "Эпоха 54: 103 / 130\n",
      "Эпоха 55: 105 / 130\n",
      "Эпоха 56: 106 / 130\n",
      "Эпоха 57: 105 / 130\n",
      "Эпоха 58: 107 / 130\n",
      "Эпоха 59: 103 / 130\n",
      "Эпоха 60: 106 / 130\n",
      "Эпоха 61: 105 / 130\n",
      "Эпоха 62: 105 / 130\n",
      "Эпоха 63: 111 / 130\n",
      "Эпоха 64: 104 / 130\n",
      "Эпоха 65: 113 / 130\n",
      "Эпоха 66: 111 / 130\n",
      "Эпоха 67: 114 / 130\n",
      "Эпоха 68: 109 / 130\n",
      "Эпоха 69: 109 / 130\n",
      "Эпоха 70: 113 / 130\n",
      "Эпоха 71: 111 / 130\n",
      "Эпоха 72: 110 / 130\n",
      "Эпоха 73: 110 / 130\n",
      "Эпоха 74: 112 / 130\n",
      "Эпоха 75: 110 / 130\n",
      "Эпоха 76: 111 / 130\n",
      "Эпоха 77: 112 / 130\n",
      "Эпоха 78: 110 / 130\n",
      "Эпоха 79: 113 / 130\n",
      "Эпоха 80: 112 / 130\n",
      "Эпоха 81: 113 / 130\n",
      "Эпоха 82: 110 / 130\n",
      "Эпоха 83: 113 / 130\n",
      "Эпоха 84: 115 / 130\n",
      "Эпоха 85: 113 / 130\n",
      "Эпоха 86: 111 / 130\n",
      "Эпоха 87: 110 / 130\n",
      "Эпоха 88: 112 / 130\n",
      "Эпоха 89: 113 / 130\n",
      "Эпоха 90: 113 / 130\n",
      "Эпоха 91: 113 / 130\n",
      "Эпоха 92: 113 / 130\n",
      "Эпоха 93: 112 / 130\n",
      "Эпоха 94: 113 / 130\n",
      "Эпоха 95: 113 / 130\n",
      "Эпоха 96: 111 / 130\n",
      "Эпоха 97: 113 / 130\n",
      "Эпоха 98: 113 / 130\n",
      "Эпоха 99: 112 / 130\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "721b63e7b58b4525b88ed4e93adb650b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=6, continuous_update=False, description='1st inner layer: ', max=10), In…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class Network:\n",
    "\n",
    "    def __init__(self, sizes, output=True):\n",
    "        \"\"\"\n",
    "        Список ``sizes`` содержит количество нейронов в соответствующих слоях\n",
    "        нейронной сети. К примеру, если бы этот лист выглядел как [2, 3, 1],\n",
    "        то мы бы получили трёхслойную нейросеть, с двумя нейронами в первом\n",
    "        (входном), тремя нейронами во втором (промежуточном) и одним нейроном\n",
    "        в третьем (выходном, внешнем) слое. Смещения и веса для нейронных сетей\n",
    "        инициализируются случайными значениями, подчиняющимися стандартному нормальному\n",
    "        распределению. Обратите внимание, что первый слой подразумевается слоем, \n",
    "        принимающим входные данные, поэтому мы не будем добавлять к нему смещение \n",
    "        (делать это не принято, поскольку смещения используются только при \n",
    "        вычислении выходных значений нейронов последующих слоёв)\n",
    "        \"\"\"\n",
    "\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x)\n",
    "                        for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "        self.output = output\n",
    "\n",
    "    def feedforward(self, a):\n",
    "        \"\"\"\n",
    "        Вычислить и вернуть выходную активацию нейронной сети\n",
    "        при получении ``a`` на входе (бывшее forward_pass).\n",
    "        \"\"\"\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = sigmoid(np.dot(w, a)+b)\n",
    "        return a\n",
    "\n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta,\n",
    "            test_data=None):\n",
    "        \"\"\"\n",
    "        Обучить нейронную сеть, используя алгоритм стохастического\n",
    "        (mini-batch) градиентного спуска. \n",
    "        ``training_data`` - лист кортежей вида ``(x, y)``, где \n",
    "        x - вход обучающего примера, y - желаемый выход (в формате one-hot). \n",
    "        Роль остальных обязательных параметров должна быть понятна из их названия.\n",
    "        Если предоставлен опциональный аргумент ``test_data``, \n",
    "        то после каждой эпохи обучения сеть будет протестирована на этих данных \n",
    "        и промежуточный результат обучения будет выведен в консоль. \n",
    "        ``test_data`` -- это список кортежей из входных данных \n",
    "        и номеров правильных классов примеров (т.е. argmax(y),\n",
    "        если y -- набор ответов в той же форме, что и в тренировочных данных).\n",
    "        Тестирование полезно для мониторинга процесса обучения,\n",
    "        но может существенно замедлить работу программы.\n",
    "        \"\"\"\n",
    "\n",
    "        if test_data is not None: n_test = len(test_data)\n",
    "        n = len(training_data)\n",
    "        success_tests = 0\n",
    "        for j in range(epochs):\n",
    "            random.shuffle(training_data)\n",
    "            mini_batches = [\n",
    "                training_data[k:k+mini_batch_size]\n",
    "                for k in range(0, n, mini_batch_size)]\n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(mini_batch, eta)\n",
    "            if test_data is not None and self.output:\n",
    "                success_tests = self.evaluate(test_data)\n",
    "                print(\"Эпоха {0}: {1} / {2}\".format(\n",
    "                    j, success_tests, n_test))\n",
    "            elif self.output:\n",
    "                print(\"Эпоха {0} завершена\".format(j))\n",
    "        if test_data is not None:\n",
    "            return success_tests / n_test\n",
    "\n",
    "    def update_mini_batch(self, mini_batch, eta):\n",
    "        \"\"\"\n",
    "        Обновить веса и смещения нейронной сети, сделав шаг градиентного\n",
    "        спуска на основе алгоритма обратного распространения ошибки, примененного\n",
    "        к одному mini batch.\n",
    "        ``mini_batch`` - список кортежей вида ``(x, y)``,\n",
    "        ``eta`` - величина шага (learning rate).\n",
    "        \"\"\"\n",
    "        \n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        for x, y in mini_batch:\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "            \n",
    "        eps = eta / len(mini_batch)\n",
    "        self.weights = [w - eps * nw for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.biases  = [b - eps * nb for b, nb in zip(self.biases,  nabla_b)]\n",
    "        \n",
    "    def backprop(self, x, y):\n",
    "        \"\"\"\n",
    "        Возвращает кортеж ``(nabla_b, nabla_w)`` -- градиент целевой функции по всем параметрам сети.\n",
    "        ``nabla_b`` и ``nabla_w`` -- послойные списки массивов ndarray,\n",
    "        такие же, как self.biases и self.weights соответственно.\n",
    "        \"\"\"\n",
    "        # Эту функцию необходимо реализовать\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "\n",
    "        # прямое распространение (forward pass)\n",
    "        a = [np.zeros((s,1)) for s in self.sizes]\n",
    "        a[0] = np.array(x)\n",
    "\n",
    "\n",
    "        for i, b, w in zip(range(1,len(a)),self.biases, self.weights):\n",
    "            a[i] = sigmoid(np.dot(w,a[i-1]) + b)\n",
    "\n",
    "        delta =  (a[-1] - np.array(y)) * a[-1]*(1-a[-1])# ошибка выходного слоя\n",
    "        nabla_b[-1] =  delta # производная J по смещениям выходного слоя\n",
    "        nabla_w[-1] =  delta.dot(a[-2].T) # производная J по весам выходного слоя\n",
    "\n",
    "        for l in range(2, self.num_layers):    \n",
    "            delta = self.weights[-l+1].T.dot(delta) * a[-l]*(1-a[-l])\n",
    "            nabla_b[-l] =  delta# производная J по смещениям L-l-го слоя\n",
    "            nabla_w[-l] =  delta.dot(a[-l-1].T)# производная J по весам L-l-го слоя\n",
    "\n",
    "        return nabla_b, nabla_w\n",
    "\n",
    "    def evaluate(self, test_data):\n",
    "        \"\"\"\n",
    "        Вернуть количество тестовых примеров, для которых нейронная сеть\n",
    "        возвращает правильный ответ. Обратите внимание: подразумевается,\n",
    "        что выход нейронной сети - это индекс, указывающий, какой из нейронов\n",
    "        последнего слоя имеет наибольшую активацию.\n",
    "        \"\"\"\n",
    "        test_results = [(np.argmax(self.feedforward(x)), y)\n",
    "                        for (x, y) in test_data]\n",
    "        return sum(int(x == y) for (x, y) in test_results)\n",
    "\n",
    "    def cost_derivative(self, output_activations, y):\n",
    "        \"\"\"\n",
    "        Возвращает вектор частных производных (\\partial C_x) / (\\partial a) \n",
    "        целевой функции по активациям выходного слоя.\n",
    "        \"\"\"\n",
    "        return (output_activations-y)\n",
    "    \n",
    "data = np.loadtxt(\"data.csv\", delimiter=\",\")\n",
    "\n",
    "means = data.mean(axis=0)\n",
    "means[-1] = 0  # правильные ответы мы нормализовывать не будем: это качественные переменные\n",
    "stds = data.std(axis=0)\n",
    "stds[-1] = 1\n",
    "data = (data - means) / stds\n",
    "\n",
    "np.random.seed(42)\n",
    "test_index = np.random.choice([True, False], len(data), replace=True, p=[0.25, 0.75])\n",
    "test  = data[test_index]\n",
    "train = data[np.logical_not(test_index)]\n",
    "\n",
    "# eye - чтобы создать вертикальный вектор, аналогичный тому, который будет выдавать нейросеть на выходе\n",
    "train = [(d[:3][:, np.newaxis], np.eye(3, 1, k=-int(d[-1]))) for d in train]  \n",
    "test =  [(d[:3][:, np.newaxis], d[-1]) for d in test]\n",
    "\n",
    "input_count  = 3  # 3 нейрона входного слоя\n",
    "hidden_count = 6  # 6 нейронов внутреннего слоя\n",
    "output_count = 3  # 3 нейрона выходного слоя, по индикатору для каждого из классов \"недолёт\", \"попал\" и \"перелёт\"\n",
    "\n",
    "random.seed(1)\n",
    "np.random.seed(1)\n",
    "nn = Network([input_count, hidden_count, output_count])\n",
    "nn.SGD(training_data=train, epochs=100, mini_batch_size=5, eta=1, test_data=test)\n",
    "\n",
    "\n",
    "\n",
    "class RegularizedNetwork(Network):\n",
    "    def __init__(self, sizes, output=True, l1=0, l2=0):\n",
    "        super().__init__(sizes, output)\n",
    "        self.l1 = l1\n",
    "        self.l2 = l2\n",
    "        \n",
    "    def update_mini_batch(self, mini_batch, eta):\n",
    "        \"\"\"\n",
    "        Обновить веса и смещения нейронной сети, сделав шаг градиентного\n",
    "        спуска на основе алгоритма обратного распространения ошибки, примененного\n",
    "        к одному mini batch. Учесть штрафы за L1 и L2.\n",
    "        ``mini_batch`` - список кортежей вида ``(x, y)``,\n",
    "        ``eta`` - величина шага (learning rate).\n",
    "        \"\"\"\n",
    "        \n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        for x, y in mini_batch:\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "            \n",
    "        eps = eta / len(mini_batch)\n",
    "        self.weights = [w - eps * nw - self.l1 * np.sign(w) - self.l2 * w for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.biases  = [b - eps * nb for b, nb in zip(self.biases,  nabla_b)]\n",
    "        \n",
    "%matplotlib inline\n",
    "from ipywidgets import *\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "@interact(layer1=IntSlider(min=0, max=10, continuous_update=False, description=\"1st inner layer: \", value=6),\n",
    "          layer2=IntSlider(min=0, max=10, continuous_update=False, description=\"2nd inner layer:\"),\n",
    "          layer3=IntSlider(min=0, max=10, continuous_update=False, description=\"3rd inner layer: \"),\n",
    "          batch_size=BoundedIntText(min=1, max=len(data), value=5, description=\"Batch size: \"),\n",
    "          learning_rate=Dropdown(options=[\"0.01\", \"0.05\", \"0.1\", \"0.5\", \"1\", \"5\", \"10\"], \n",
    "                                 value=\"1\", description=\"Learning rate: \"),\n",
    "          l1=Dropdown(options=[\"0\", \"0.0001\", \"0.0005\", \"0.001\", \"0.005\", \"0.01\", \"0.05\", \"0.1\"], value=\"0.005\", \n",
    "                      description=\"$\\ell_1:$\"),\n",
    "          l2=Dropdown(options=[\"0\", \"0.0001\", \"0.0005\", \"0.001\", \"0.005\", \"0.01\", \"0.05\", \"0.1\"], value=\"0.005\", \n",
    "                      description=\"$\\ell_2:$\")\n",
    "         )\n",
    "def learning_curve_by_network_structure_and_regularization(layer1, layer2, layer3, batch_size, learning_rate, l1, l2):\n",
    "    layers = [x for x in [input_count, layer1, layer2, layer3, output_count] if x > 0]\n",
    "    nn = RegularizedNetwork(layers, output=False, l1=float(l1), l2=float(l2))\n",
    "    learning_rate=float(learning_rate)\n",
    "    \n",
    "    CER = []\n",
    "    cost_train = []\n",
    "    cost_test  = []\n",
    "    for _ in range(150):\n",
    "        nn.SGD(training_data=train, epochs=1, mini_batch_size=batch_size, eta=learning_rate)\n",
    "        CER.append(1 - nn.evaluate(test) / len(test))\n",
    "        cost_test.append(cost_function(nn, test, onehot=False))\n",
    "        cost_train.append(cost_function(nn, train, onehot=True))\n",
    "    \n",
    "    fig = plt.figure(figsize=(15,5))\n",
    "    fig.add_subplot(1,2,1)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.plot(CER)\n",
    "    plt.title(\"Classification error rate\")\n",
    "    plt.ylabel(\"Percent of incorreclty identified observations\")\n",
    "    plt.xlabel(\"Epoch number\")\n",
    "    \n",
    "    fig.add_subplot(1,2,2)\n",
    "    plt.plot(cost_train, label=\"Training error\", color=\"orange\")\n",
    "    plt.plot(cost_test, label=\"Test error\", color=\"blue\")\n",
    "    plt.title(\"Learning curve\")\n",
    "    plt.ylabel(\"Cost function\")\n",
    "    plt.xlabel(\"Epoch number\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
