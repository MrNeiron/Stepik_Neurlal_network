{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w: \n",
      " (2, 3, 1)\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Incorrect weight shape",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-91-1737f1bcd11f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[1;31m#layer2 = np.array([ Neuron(w[0]),  Neuron(w[1])])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"w: \\n\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m \u001b[0mlayer2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNeuron\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m \u001b[0mlayer3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNeuron\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-91-1737f1bcd11f>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, weights, activation_function, activation_function_derivative)\u001b[0m\n\u001b[0;32m     76\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation_function_derivative\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msigmoid_prime\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m         \u001b[1;32massert\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Incorrect weight shape\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: Incorrect weight shape"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#Активационная функция сигмоидального нейрона\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "    \n",
    "#Производная активационной функции сигмоидального нейрона\n",
    "def sigmoid_prime(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "#Вычисляет вектор частных производных целевой функции по каждому из предсказаний(используется при расчете градиента)\n",
    "def J_quadratic_derivative(y, y_hat):\n",
    "\n",
    "    assert y_hat.shape == y.shape and y_hat.shape[1] == 1, 'Incorrect shapes'\n",
    "    \n",
    "    return (y_hat - y) / len(y)\n",
    "    \n",
    "    \n",
    "#Рассчитывает градиент(аналитическая производная целевой функции)\n",
    "def compute_grad_analytically(neuron, X, y, J_prime=J_quadratic_derivative):\n",
    "    \n",
    "    # Вычисляем активации\n",
    "    # z - вектор результатов сумматорной функции нейрона на разных примерах\n",
    "    \n",
    "    z = neuron.summatory(X)\n",
    "    y_hat = neuron.activation(z)\n",
    "\n",
    "    # Вычисляем нужные нам частные производные\n",
    "    dy_dyhat = J_prime(y, y_hat)\n",
    "    dyhat_dz = neuron.activation_function_derivative(z)\n",
    "    \n",
    "    # осознайте эту строчку:\n",
    "    dz_dw = X\n",
    "\n",
    "    \n",
    "\n",
    "    # а главное, эту:\n",
    "    grad = ((dy_dyhat * dyhat_dz).T).dot(dz_dw)\n",
    "    \n",
    "    # можно было написать в два этапа. Осознайте, почему получается одно и то же\n",
    "    #2)grad_matrix = dy_dyhat * dyhat_dz * dz_dw\n",
    "    #2)grad = np.sum(grad_matrix, axis=0)\n",
    "    \n",
    "    # Сделаем из горизонтального вектора вертикальный\n",
    "    \n",
    "    \n",
    "    grad = grad.T\n",
    "    #2)grad.shape = (5,1)\n",
    "    \n",
    "    return grad\n",
    "\n",
    "def compute_error_analytically(neuron, X, y, J_prime=J_quadratic_derivative):\n",
    "     # Вычисляем активации\n",
    "    # z - вектор результатов сумматорной функции нейрона на разных примерах\n",
    "    \n",
    "    z = neuron.summatory(X)\n",
    "    y_hat = neuron.activation(z)\n",
    "\n",
    "    # Вычисляем нужные нам частные производные\n",
    "    dy_dyhat = J_prime(y, y_hat)\n",
    "    dyhat_dz = neuron.activation_function_derivative(z)\n",
    "    \n",
    "\n",
    "    # а главное, эту:\n",
    "    error = (dy_dyhat * dyhat_dz).T\n",
    "\n",
    "    # Сделаем из горизонтального вектора вертикальный\n",
    "    \n",
    "    \n",
    "    error = error.T\n",
    "    \n",
    "    return error\n",
    "\n",
    "class Neuron:\n",
    "\n",
    "    def __init__(self, weights, activation_function = sigmoid, activation_function_derivative = sigmoid_prime):\n",
    "        \n",
    "        assert weights.shape[1] == 1, \"Incorrect weight shape\"\n",
    "        \n",
    "        self.w = weights\n",
    "        self.activation_function = activation_function\n",
    "        self.activation_function_derivative = activation_function_derivative\n",
    "        self.error = 0\n",
    "\n",
    "    #Сумматорный метод\n",
    "    def summatory(self, input_matrix):\n",
    "        return input_matrix.dot(self.w)\n",
    "        \n",
    "    #Активационный метод\n",
    "    def activation(self, summatory_activation):\n",
    "        #print(\"s: \", self.activation_function(summatory_activation))\n",
    "        return self.activation_function(summatory_activation)\n",
    "        \n",
    "    #Векторизованный метод рассчета предсказанных значений(гораздо лучший способ)\n",
    "    def vectorized_forward_pass(self, input_matrix):\n",
    "        return self.activation(self.summatory(input_matrix))\n",
    "    \n",
    "    \n",
    "    #Обучение весов с помощью одного батча(несколько случайных примеров из всех имеющихся примеров)\n",
    "    def update_mini_batch(self, X, y, learning_rate, eps):\n",
    "        \n",
    "        before = J_quadratic(self, X, y)\n",
    "        \n",
    "        #self.w -= learning_rate * compute_grad_analytically(self,X,y)\n",
    "        self.w -= learning_rate * compute_grad_numerically_2(self,X,y)\n",
    "\n",
    "        \n",
    "        return np.abs(before-J_quadratic(self, X, y)) < eps\n",
    "    \n",
    "    \n",
    "    #Обучение весов с помощью батчей\n",
    "    def SGD(self, X, y, batch_size, learning_rate =0.1, eps=1e-6, max_steps = 200):\n",
    "        cur_step = 0\n",
    "        #for _ in range(max_steps):\n",
    "        while cur_step < max_steps:\n",
    "            cur_step += 1\n",
    "            batch = np.random.choice(len(X), batch_size, replace = False)\n",
    "            if self.update_mini_batch(X[batch], y[batch], learning_rate, eps) == 1: \n",
    "                return 1\n",
    "        return 0\n",
    "                \n",
    "    \n",
    "    def backpropagation(self, X, y, derivative = compute_grad_analytically):\n",
    "        \n",
    "        answer = derivative(X,y)\n",
    "        \n",
    "        return answer\n",
    "    \n",
    "    def backpropagation2(self, X, y, derivative = compute_grad_analytically):\n",
    "        \n",
    "        print(np.shape(self.w.reshape(1,len(self.w))))\n",
    "        answer1 = self.activation_function_derivative(self.summatory(X))\n",
    "        print(\"X: \", np.shape(X))\n",
    "        print(\"X2: \", np.shape(self.vectorized_forward_pass(X)))\n",
    "        answer2 = derivative(self, self.vectorized_forward_pass(X), y)\n",
    "        #answer3 = self.w.reshape(1,len(self.w))\n",
    "        #answer4 = answer3 * answer2 * answer1\n",
    "        print(answer2)\n",
    "        #return answer2\n",
    "\n",
    "\n",
    "    def teach1(self, X, y, derivative = compute_error_analytically, learning_steps = 10, max_steps = 1e8):\n",
    "        \n",
    "        self.error = derivative(self,X,y).reshape(1,len(X))\n",
    "        answer = np.transpose(self.w.dot(self.error) * self.activation_function_derivative(self.summatory(X)).reshape(1,len(X)))\n",
    "        #self.w -= answer\n",
    "        return self.error\n",
    "    \n",
    "    def teach2(self, error, weights, X, learning_steps = 10, max_steps = 1e8 ):\n",
    "        \n",
    "        #answer1 = self.w.reshape(1,len(self.w))\n",
    "\n",
    "        self.error = np.reshape(weights.dot(error.reshape(1,len(X))) * self.activation_function_derivative(self.summatory(X)).reshape(1,len(X)), (len(X),1))\n",
    "        answer = self.w*self.error.reshape(1,len(X))*self.activation_function_derivative(self.summatory(X)).reshape(1,len(X))\n",
    "        #self.w -= answer\n",
    "        return self.error\n",
    "    \n",
    "def get_error(deltas, sums, weights):\n",
    "\n",
    "    return (weights.T.dot(deltas.reshape(1,deltas.shape[0])) * sigmoid_prime(sums.T)).mean(axis = 1)#более быстрое решение\n",
    "    #return (deltas.dot(weights) * sigmoid_prime(sums)).mean(axis = 0).T\n",
    "    \n",
    "'''def FindAnswer(l1, l2, l3, y, learning_steps = 1):\n",
    "\n",
    "    step = 0\n",
    "    while step < learning_steps:\n",
    "        step += 1\n",
    "        #for i in range(len(l2)):\n",
    "        #    decide2[i] = l2[i].vectorized_forward_pass(l1).reshape(1,len(l1))\n",
    "        decide2 = np.array([(l2[i].vectorized_forward_pass(l1)) for i in range(len(l2))]).reshape(len(l2),len(l1)).T\n",
    "        #print(decide2)\n",
    "        decide3 = l3.vectorized_forward_pass(decide2)\n",
    "        learning3 = l3.teach1(decide2, y)\n",
    "\n",
    "        errorl2 = get_error(learning3.reshape(7,1), np.hstack((l2[0].summatory(l1),l2[1].summatory(l1))), l3.w.T)\n",
    "        print(\"error: \\n\",errorl2)\n",
    "        learning2 = np.array([l2[i].teach2(learning3, l3.w[0], l1) for i in range(len(l2))]).reshape(len(l2),len(l1))\n",
    "        #return layer3.teach1(np.array([(l2[i].teach2(x,y[i])) for i in range(len(l2))]).T, y[2], y).reshape(len(l1),1)\n",
    "    return decide3'''\n",
    "\n",
    "def FindWeight(l1,l2,l3,y):\n",
    "    weights = np.hstack((l2[0].w,l2[1].w))\n",
    "    sumsL2 = l2.summatory(l1)\n",
    "\n",
    "    \n",
    "w = np.random.randint(-5,5,(2,3))\n",
    "w = np.array([\n",
    "                [[0.79], [0.44], [0.85]],\n",
    "                [[0.85], [0.43], [0.29]]\n",
    "            ])\n",
    "\n",
    "w1 = np.array([[0.5],[0.52]])\n",
    "w = np.array([[[2],[2],[2]], [[2],[2],[2]]])\n",
    "x = np.array([[0,0,0],\n",
    "               [0,0,1],\n",
    "               [0,1,0],\n",
    "               [1,0,0],\n",
    "               [0,1,1],\n",
    "               [1,1,0],\n",
    "               [1,1,1]])\n",
    "#x = np.array([[1,1,0]])\n",
    "x = np.array([[0,1,2]])\n",
    "y = np.array([[0],[1],[0],[0],[1],[0],[1]])\n",
    "#y = np.array([[0]])\n",
    "y = np.array([[1]])\n",
    "#layer2 = np.array([ Neuron(w[0]),  Neuron(w[1])])\n",
    "print(\"w: \\n\", w.shape)\n",
    "layer2 = Neuron(w)\n",
    "layer3 = Neuron(w1)\n",
    "\n",
    "#print(FindAnswer(x, layer2, layer3, y))\n",
    "print(FindWeight(x, layer2, layer3, y))\n",
    "#print(layer2[0].backpropagation2(x, y[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
