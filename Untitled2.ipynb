{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "(3,)\n",
      "3\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Incorrect weight shape",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-eb8c6e6fa4fc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    143\u001b[0m               0]])\n\u001b[0;32m    144\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 145\u001b[1;33m \u001b[0mlayer1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNeuron\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    146\u001b[0m \u001b[0mlayer2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m \u001b[0mNeuron\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mNeuron\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[0mlayer3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNeuron\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-28-eb8c6e6fa4fc>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, weights, activation_function, activation_function_derivative)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m         \u001b[1;32massert\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Incorrect weight shape\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: Incorrect weight shape"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#Активационная функция сигмоидального нейрона\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "    \n",
    "#Производная активационной функции сигмоидального нейрона\n",
    "def sigmoid_prime(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "#Вычисляет вектор частных производных целевой функции по каждому из предсказаний(используется при расчете градиента)\n",
    "def J_quadratic_derivative(y, y_hat):\n",
    "\n",
    "    assert y_hat.shape == y.shape and y_hat.shape[1] == 1, 'Incorrect shapes'\n",
    "    \n",
    "    return (y_hat - y) / len(y)\n",
    "    \n",
    "    \n",
    "#Рассчитывает градиент(аналитическая производная целевой функции)\n",
    "def compute_grad_analytically(neuron, X, y, J_prime=J_quadratic_derivative):\n",
    "    \n",
    "    # Вычисляем активации\n",
    "    # z - вектор результатов сумматорной функции нейрона на разных примерах\n",
    "    \n",
    "    z = neuron.summatory(X)\n",
    "    y_hat = neuron.activation(z)\n",
    "\n",
    "    # Вычисляем нужные нам частные производные\n",
    "    dy_dyhat = J_prime(y, y_hat)\n",
    "    dyhat_dz = neuron.activation_function_derivative(z)\n",
    "    \n",
    "    \n",
    "    # осознайте эту строчку:\n",
    "    dz_dw = X\n",
    "\n",
    "    \n",
    "\n",
    "    # а главное, эту:\n",
    "    grad = ((dy_dyhat * dyhat_dz).T).dot(dz_dw)\n",
    "    \n",
    "    # можно было написать в два этапа. Осознайте, почему получается одно и то же\n",
    "    #2)grad_matrix = dy_dyhat * dyhat_dz * dz_dw\n",
    "    #2)grad = np.sum(grad_matrix, axis=0)\n",
    "    \n",
    "    # Сделаем из горизонтального вектора вертикальный\n",
    "    \n",
    "    \n",
    "    grad = grad.T\n",
    "    #2)grad.shape = (5,1)\n",
    "    \n",
    "    return grad\n",
    "\n",
    "class Neuron:\n",
    "\n",
    "    def __init__(self, weights, activation_function = sigmoid, activation_function_derivative = sigmoid_prime):\n",
    "        \n",
    "        print(weights.shape[1])\n",
    "        assert weights.shape[1] == 1, \"Incorrect weight shape\"\n",
    "        \n",
    "        self.w = weights\n",
    "        self.activation_function = activation_function\n",
    "        self.activation_function_derivative = activation_function_derivative\n",
    "\n",
    "    \n",
    "    #Сумматорный метод\n",
    "    def summatory(self, input_matrix):\n",
    "        return input_matrix.dot(self.w)\n",
    "        \n",
    "    #Активационный метод\n",
    "    def activation(self, summatory_activation):\n",
    "        return self.activation_function(summatory_activation)\n",
    "        \n",
    "    #Векторизованный метод рассчета предсказанных значений(гораздо лучший способ)\n",
    "    def vectorized_forward_pass(self, input_matrix):\n",
    "        return self.activation(self.summatory(input_matrix))\n",
    "    \n",
    "    \n",
    "    #Обучение весов с помощью одного батча(несколько случайных примеров из всех имеющихся примеров)\n",
    "    def update_mini_batch(self, X, y, learning_rate, eps):\n",
    "        \n",
    "        before = J_quadratic(self, X, y)\n",
    "        \n",
    "        #self.w -= learning_rate * compute_grad_analytically(self,X,y)\n",
    "        self.w -= learning_rate * compute_grad_numerically_2(self,X,y)\n",
    "\n",
    "        \n",
    "        return np.abs(before-J_quadratic(self, X, y)) < eps\n",
    "    \n",
    "    \n",
    "    #Обучение весов с помощью батчей\n",
    "    def SGD(self, X, y, batch_size, learning_rate =0.1, eps=1e-6, max_steps = 200):\n",
    "        cur_step = 0\n",
    "        #for _ in range(max_steps):\n",
    "        while cur_step < max_steps:\n",
    "            cur_step += 1\n",
    "            batch = np.random.choice(len(X), batch_size, replace = False)\n",
    "            if self.update_mini_batch(X[batch], y[batch], learning_rate, eps) == 1: \n",
    "                return 1\n",
    "        return 0\n",
    "                \n",
    "    def teach(self, input_matrix, y, learning_steps = 10, max_steps = 1e8):\n",
    "        \n",
    "        for i in range(learning_steps):\n",
    "            self.train_until_convergence(input_matrix, y, max_steps)\n",
    "            \n",
    "        return self.vectorized_forward_pass(input_matrix)\n",
    "    \n",
    "    def backpropagation(self, X, y, derivative = compute_grad_analytically):\n",
    "        print(np.shape(self.w.reshape(1,len(self.w))))\n",
    "        #answer = self.w.reshape(1,len(self.w)) * derivative(self, X, y)*self.acivation_function_derivative(self.summatory(X))\n",
    "        \n",
    "        return answer\n",
    "\n",
    "\n",
    "def FindAnswer(l1, l2, l3, learning_steps = 10):\n",
    "    \n",
    "    return layer3.teach(np.array([(l2[i].teach(x,y[i])) for i in range(len(l2))]).T, y[2]).reshape(len(l1),1)\n",
    "\n",
    "w = np.random.randint(-5,5,(2,3))\n",
    "print(w.shape[1])\n",
    "'''w = np.array([\n",
    "                [0.79, 0.44, 0.85],\n",
    "                [0.85, 0.43, 0.29]\n",
    "            ])'''\n",
    "\n",
    "w1 = np.array([[0.5,0.52]])\n",
    "x = np.array([[0,0],\n",
    "               [0,1],\n",
    "               [1,0],\n",
    "               [1,1]])\n",
    "\n",
    "y = np.array([[0,\n",
    "               1,\n",
    "               0,\n",
    "               0],\n",
    "               [0,\n",
    "               0,\n",
    "               1,\n",
    "               0],\n",
    "              [0,\n",
    "              1,\n",
    "              1,\n",
    "              0]])\n",
    "print(np.shape(w[0]))\n",
    "layer1 = Neuron(np.array([[0,0,0]]))\n",
    "layer2 = np.array([ Neuron(w),  Neuron(w)])\n",
    "layer3 = Neuron(w1.T,1)\n",
    "\n",
    "layer2[0].backpropagation(x, y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
